{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logreg_w/o_BC.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "1fjBO8gmmk-VhAtJQKwR0Vey4_g8IkDLK",
          "timestamp": 1523548094268
        },
        {
          "file_id": "1IgUyNIJnQ3XiOs5MXUyMgGiQyj6fA9C_",
          "timestamp": 1523508111065
        },
        {
          "file_id": "1AW__DS4maajMeBMC6MUouzUb3cMTpiUe",
          "timestamp": 1523212800809
        }
      ],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "gffv_D53ng4Z",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "beIOasm-neMg",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as utils\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "from random import shuffle\n",
        "from collections import defaultdict\n",
        "from copy import deepcopy\n",
        "from itertools import chain\n",
        "\n",
        "from __future__ import print_function\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YeiBZDE-Gfji",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "06f94cfb-4124-4076-d514-d15bfdf1756c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1524268205606,
          "user_tz": 240,
          "elapsed": 301,
          "user": {
            "displayName": "Hamed Layeghi",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107778543655020009707"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "hyperparam_tuning_amsgrad = False \n",
        "hyperparam_tuning_adam = False\n",
        "num_epochs_hyp = 10 \n",
        "\n",
        "num_epochs = 13\n",
        "\n",
        "valid_size = 0\n",
        "num_train_hyp = 250*128 #train size for hyperparameter tuning\n",
        "num_valid_hyp = 20*128 # valid size for hp tuning\n",
        "batch_size = 128\n",
        "# Learning the parameters using Amsgrad optimization with categorical cross-entropy loss. The learning rate is initially set to 0.001.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "alphas = np.logspace(-1,-2.5,8)\n",
        "betas = np.linspace(0.994,0.996,3)\n",
        "alphas_adam = np.logspace(0,-2,9)[1:]\n",
        "betas_adam = np.linspace(0.99,0.999,3)\n",
        "\n",
        "print('alphas=',alphas)\n",
        "print('betas=',betas)\n",
        "print('alphas_adam=',alphas_adam)\n",
        "print('betas_adam=',betas_adam)\n",
        "\n"
      ],
      "execution_count": 402,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "alphas= [0.1        0.06105402 0.03727594 0.02275846 0.01389495 0.00848343\n",
            " 0.00517947 0.00316228]\n",
            "betas= [0.994 0.995 0.996]\n",
            "alphas_adam= [0.56234133 0.31622777 0.17782794 0.1        0.05623413 0.03162278\n",
            " 0.01778279 0.01      ]\n",
            "betas_adam= [0.99   0.9945 0.999 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q-YjangfkCyc",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# The below code implements Optimizer class and is taken from pytorch's github implementation\n",
        "\n",
        "required = object()\n",
        "\n",
        "\n",
        "class Optimizer(object):\n",
        "    \"\"\"Base class for all optimizers.\n",
        "\n",
        "    Arguments:\n",
        "        params (iterable): an iterable of :class:`Variable` s or\n",
        "            :class:`dict` s. Specifies what Variables should be optimized.\n",
        "        defaults: (dict): a dict containing default values of optimization\n",
        "            options (used when a parameter group doesn't specify them).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, defaults):\n",
        "        self.defaults = defaults\n",
        "\n",
        "        if isinstance(params, Variable) or torch.is_tensor(params):\n",
        "            raise TypeError(\"params argument given to the optimizer should be \"\n",
        "                            \"an iterable of Variables or dicts, but got \" +\n",
        "                            torch.typename(params))\n",
        "\n",
        "        self.state = defaultdict(dict)\n",
        "        self.param_groups = []\n",
        "\n",
        "        param_groups = list(params)\n",
        "        if len(param_groups) == 0:\n",
        "            raise ValueError(\"optimizer got an empty parameter list\")\n",
        "        if not isinstance(param_groups[0], dict):\n",
        "            param_groups = [{'params': param_groups}]\n",
        "\n",
        "        for param_group in param_groups:\n",
        "            self.add_param_group(param_group)\n",
        "\n",
        "    def __getstate__(self):\n",
        "        return {\n",
        "            'state': self.state,\n",
        "            'param_groups': self.param_groups,\n",
        "        }\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        self.__dict__.update(state)\n",
        "\n",
        "    def state_dict(self):\n",
        "        \"\"\"Returns the state of the optimizer as a :class:`dict`.\n",
        "\n",
        "        It contains two entries:\n",
        "\n",
        "        * state - a dict holding current optimization state. Its content\n",
        "            differs between optimizer classes.\n",
        "        * param_groups - a dict containing all parameter groups\n",
        "        \"\"\"\n",
        "        # Save ids instead of Variables\n",
        "        def pack_group(group):\n",
        "            packed = {k: v for k, v in group.items() if k != 'params'}\n",
        "            packed['params'] = [id(p) for p in group['params']]\n",
        "            return packed\n",
        "        param_groups = [pack_group(g) for g in self.param_groups]\n",
        "        # Remap state to use ids as keys\n",
        "        packed_state = {(id(k) if isinstance(k, Variable) else k): v\n",
        "                        for k, v in self.state.items()}\n",
        "        return {\n",
        "            'state': packed_state,\n",
        "            'param_groups': param_groups,\n",
        "        }\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        \"\"\"Loads the optimizer state.\n",
        "\n",
        "        Arguments:\n",
        "            state_dict (dict): optimizer state. Should be an object returned\n",
        "                from a call to :meth:`state_dict`.\n",
        "        \"\"\"\n",
        "        # deepcopy, to be consistent with module API\n",
        "        state_dict = deepcopy(state_dict)\n",
        "        # Validate the state_dict\n",
        "        groups = self.param_groups\n",
        "        saved_groups = state_dict['param_groups']\n",
        "\n",
        "        if len(groups) != len(saved_groups):\n",
        "            raise ValueError(\"loaded state dict has a different number of \"\n",
        "                             \"parameter groups\")\n",
        "        param_lens = (len(g['params']) for g in groups)\n",
        "        saved_lens = (len(g['params']) for g in saved_groups)\n",
        "        if any(p_len != s_len for p_len, s_len in zip(param_lens, saved_lens)):\n",
        "            raise ValueError(\"loaded state dict contains a parameter group \"\n",
        "                             \"that doesn't match the size of optimizer's group\")\n",
        "\n",
        "        # Update the state\n",
        "        id_map = {old_id: p for old_id, p in\n",
        "                  zip(chain(*(g['params'] for g in saved_groups)),\n",
        "                      chain(*(g['params'] for g in groups)))}\n",
        "        state = defaultdict(\n",
        "            dict, {id_map.get(k, k): v for k, v in state_dict['state'].items()})\n",
        "\n",
        "        # Update parameter groups, setting their 'params' value\n",
        "        def update_group(group, new_group):\n",
        "            new_group['params'] = group['params']\n",
        "            return new_group\n",
        "        param_groups = [\n",
        "            update_group(g, ng) for g, ng in zip(groups, saved_groups)]\n",
        "        self.__setstate__({'state': state, 'param_groups': param_groups})\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"\"\"Clears the gradients of all optimized :class:`Variable` s.\"\"\"\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is not None:\n",
        "                    if p.grad.volatile:\n",
        "                        p.grad.data.zero_()\n",
        "                    else:\n",
        "                        data = p.grad.data\n",
        "                        p.grad = Variable(data.new().resize_as_(data).zero_())\n",
        "\n",
        "    def step(self, closure):\n",
        "        \"\"\"Performs a single optimization step (parameter update).\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable): A closure that reevaluates the model and\n",
        "                returns the loss. Optional for most optimizers.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def add_param_group(self, param_group):\n",
        "        \"\"\"Add a param group to the :class:`Optimizer` s `param_groups`.\n",
        "\n",
        "        This can be useful when fine tuning a pre-trained network as frozen layers can be made\n",
        "        trainable and added to the :class:`Optimizer` as training progresses.\n",
        "\n",
        "        Arguments:\n",
        "            param_group (dict): Specifies what Variables should be optimized along with group\n",
        "            specific optimization options.\n",
        "        \"\"\"\n",
        "        assert isinstance(param_group, dict), \"param group must be a dict\"\n",
        "\n",
        "        params = param_group['params']\n",
        "        if isinstance(params, Variable):\n",
        "            param_group['params'] = [params]\n",
        "        else:\n",
        "            param_group['params'] = list(params)\n",
        "\n",
        "        for param in param_group['params']:\n",
        "            if not isinstance(param, Variable):\n",
        "                raise TypeError(\"optimizer can only optimize Variables, \"\n",
        "                                \"but one of the params is \" + torch.typename(param))\n",
        "            if not param.requires_grad:\n",
        "                raise ValueError(\"optimizing a parameter that doesn't require gradients\")\n",
        "            if not param.is_leaf:\n",
        "                raise ValueError(\"can't optimize a non-leaf Variable\")\n",
        "\n",
        "        for name, default in self.defaults.items():\n",
        "            if default is required and name not in param_group:\n",
        "                raise ValueError(\"parameter group didn't specify a value of required optimization parameter \" +\n",
        "                                 name)\n",
        "            else:\n",
        "                param_group.setdefault(name, default)\n",
        "\n",
        "        param_set = set()\n",
        "        for group in self.param_groups:\n",
        "            param_set.update(set(group['params']))\n",
        "\n",
        "        if not param_set.isdisjoint(set(param_group['params'])):\n",
        "            raise ValueError(\"some parameters appear in more than one parameter group\")\n",
        "\n",
        "        self.param_groups.append(param_group)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "exACrNNSkDwr",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# The below code modifies pytorch implementation of Adams optimizer to implement Amsgrad algorithm \n",
        "\n",
        "class Amsgrad(Optimizer):\n",
        "    \"\"\"Implements Amsgrad algorithm.\n",
        "\n",
        "    It has been proposed in `On the Convergence of Adam and Beyond`_.\n",
        "\n",
        "    Arguments:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float, optional): learning rate (default: 1e-3)\n",
        "        betas (Tuple[float, float], optional): coefficients used for computing\n",
        "            running averages of gradient and its square (default: (0.9, 0.999))\n",
        "        eps (float, optional): term added to the denominator to improve\n",
        "            numerical stability (default: 1e-8)\n",
        "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
        "\n",
        "    .. _n the Convergence of Adam and Beyond:\n",
        "        https://openreview.net/forum?id=ryQu7f-RZ\n",
        "    \"\"\"\n",
        "\n",
        "    # constructor to initialize the hyper-parameters\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.99), eps=1e-8,\n",
        "                 weight_decay=0):\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
        "                        weight_decay=weight_decay)\n",
        "        super(Amsgrad, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Amsgrad does not support sparse gradients, please consider SparseAdam instead')\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
        "                    # Maximum exponential moving average of squared gradient values\n",
        "                    state['exp_avg_sq_maximum'] = torch.zeros_like(p.data)\n",
        "                exp_avg, exp_avg_sq, exp_avg_sq_maximum = state['exp_avg'], state['exp_avg_sq'], state['exp_avg_sq_maximum']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    grad = grad.add(group['weight_decay'], p.data)\n",
        "\n",
        "               # Decay the first and second moment running average coefficient\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                \n",
        "                # Calculate the maximum between the previous maximum exponential average of squared gradients and current value\n",
        "                exp_avg_sq_maximum = torch.max(exp_avg_sq_maximum,exp_avg_sq)\n",
        "\n",
        "                # Store the maximum\n",
        "                state['exp_avg_sq_maximum'] = exp_avg_sq_maximum\n",
        "                \n",
        "                \n",
        "                denom = exp_avg_sq_maximum.sqrt().add_(group['eps'])\n",
        "\n",
        "                step_size = group['lr']\n",
        "                \n",
        "                # Performs update on parameters of the network based on gradients\n",
        "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
        "\n",
        "        return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "68RYT3ywnpcz",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Reading MNIST dataset\n",
        "'''\n",
        "\n",
        "\n",
        "# It converts the datapoints into tensor and then normalise each datapoint with given mean and standard deviation \n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                            transforms.Normalize([0.5,], [0.5,])])\n",
        "\n",
        "# Loading the  MNIST dataset consisting of 60000 grayscale training images and 10000 grayscale testing images of size 28 X 28 pixels in a batch size of 128 images at a time\n",
        "# and performing the above transformations \n",
        "# Randomly splitting data into 50000 training examples and 10000 validation examples by generating 50000 random unique indices in the range [0,59999] and then retreiving the initial 5000 values at those indices for building the training dataset. The values at following 1000 indices are used for building validation dataset.\n",
        "trainset_mnist = datasets.MNIST('./', train=True, transform=transform, download=True)\n",
        "num_train = len(trainset_mnist)\n",
        "indices = list(range(num_train))\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "train_idx_hyp, valid_idx_hyp = indices[num_valid_hyp:num_valid_hyp+num_train_hyp], indices[:num_valid_hyp]\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "train_sampler_hyp = SubsetRandomSampler(train_idx_hyp)\n",
        "valid_sampler_hyp = SubsetRandomSampler(valid_idx_hyp)\n",
        "\n",
        "trainloader_mnist = torch.utils.data.DataLoader(trainset_mnist, batch_size=batch_size, sampler=train_sampler)\n",
        "validloader_mnist = torch.utils.data.DataLoader(trainset_mnist, batch_size=batch_size, sampler=valid_sampler)\n",
        "\n",
        "trainloader_mnist_hyp = torch.utils.data.DataLoader(trainset_mnist, batch_size=batch_size, sampler=train_sampler_hyp)\n",
        "validloader_mnist_hyp = torch.utils.data.DataLoader(trainset_mnist, batch_size=batch_size, sampler=valid_sampler_hyp)\n",
        "\n",
        "testset_mnist = torchvision.datasets.MNIST(root='./', train=False, download=True, transform=transform)\n",
        "testloader_mnist = torch.utils.data.DataLoader(testset_mnist, batch_size=128, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1IGl550ZheQ6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "num_batches_train = int((1-valid_size)*num_train/batch_size)\n",
        "num_batches_valid = int(valid_size*num_train/batch_size)\n",
        "num_batches_train_hyp = int(num_train_hyp/batch_size)\n",
        "num_batches_valid_hyp = int(num_valid_hyp/batch_size)\n",
        "num_test = len(trainset_mnist)\n",
        "num_batches_test = int(num_test/batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H1fYZhj_hhT1",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "17c3a3f2-daa5-4707-d8f4-71a75cf0b1fd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1524268207471,
          "user_tz": 240,
          "elapsed": 234,
          "user": {
            "displayName": "Hamed Layeghi",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107778543655020009707"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "num_batches_train, num_batches_valid, num_batches_train_hyp, num_batches_valid_hyp"
      ],
      "execution_count": 407,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(468, 0, 250, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 407
        }
      ]
    },
    {
      "metadata": {
        "id": "18Cn6iEtNiVX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "input_size = 784\n",
        "num_classes = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AF9T7pz_nrVV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Neural net architecture consisting of one hidden layer with 100 ReLu activation units and 1 output layer with 10 output units\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_size, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1,28*28)\n",
        "        out = self.linear(x)\n",
        "        \n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZsRMkPoyn-iM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Instanstiating the network on GPU\n",
        "net = LogisticRegression(input_size, num_classes)\n",
        "initial_state = net.state_dict()\n",
        "net = net.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gFVwYfD7hyl8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "45350870-56e1-4e28-cd20-3bf5c248f5be",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1524268209167,
          "user_tz": 240,
          "elapsed": 278,
          "user": {
            "displayName": "Hamed Layeghi",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107778543655020009707"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(net)"
      ],
      "execution_count": 411,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression(\n",
            "  (linear): Linear(in_features=784, out_features=10)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r3ZV48ptBuZp",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# The function validation_loss() calculates the loss on validation data which is shuffled and passed to the network in batch size of 128 instances after every epoch.\n",
        "def validation_loss(validloader, num_batches_valid, net_, criterion_):\n",
        "  running_valid_loss = 0.0\n",
        "  for i, data in enumerate(validloader, 0):\n",
        "      inputs, labels = data\n",
        "      inputs = inputs.float()\n",
        "      labels = labels.long()\n",
        "      inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
        "      \n",
        "      valid_output = net_(inputs)\n",
        "      \n",
        "      loss_valid = criterion_(valid_output, labels)\n",
        "\n",
        "      # print statistics\n",
        "      running_valid_loss += loss_valid.data[0]\n",
        "   \n",
        "  return (running_valid_loss / num_batches_valid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XJ-39EsWT_cE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def adjust_learning_rate(optimizer,lr,t):\n",
        "    new_lr = lr/np.sqrt(t)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = new_lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DHu-5mxfchaA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning for AMSGRAD\n",
        "if hyperparam_tuning_amsgrad:\n",
        "  train_loss_list = []\n",
        "  valid_loss_list = []\n",
        "  valid_loss_min_list = []\n",
        "  valid_loss_min_lista = []\n",
        "  valid_loss_min_listb = []\n",
        "\n",
        "  for alpha in alphas:\n",
        "    valid_loss_min_lista.append(([], alpha))\n",
        "    for beta in betas:\n",
        "      train_loss_list.append(([], alpha, beta))\n",
        "      valid_loss_list.append(([], alpha, beta))\n",
        "      valid_loss_min_list.append(([], alpha, beta))\n",
        "  for beta in betas:\n",
        "    valid_loss_min_listb.append(([], beta))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  total_combo = len(alphas)*len(betas)\n",
        "\n",
        "  param_combo_count = 0\n",
        "  alpha_count = 0\n",
        "  \n",
        "  for alpha in alphas:\n",
        "    beta_count = 0\n",
        "    for beta in betas:\n",
        "      print(\"alpha:\", alpha, \" beta 2: \", beta)\n",
        "      print(\"Parameter Combination:\", param_combo_count+1, \"/\", total_combo)\n",
        "      net.load_state_dict(initial_state)\n",
        "      optimizer = Amsgrad(net.parameters(), lr=alpha, betas=(0.9,beta))\n",
        "      start_time_combo = time.time()\n",
        "\n",
        "\n",
        "      for epoch in range(num_epochs_hyp):  # loop over the dataset multiple times\n",
        "          running_loss_train = 0.0\n",
        "          for i, data in enumerate(trainloader_mnist_hyp, 0):\n",
        "              # get the inputs\n",
        "              inputs, labels = data\n",
        "\n",
        "              # wrap them in Variable\n",
        "              inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
        "\n",
        "              # zero the parameter gradients\n",
        "              optimizer.zero_grad()\n",
        "\n",
        "              # forward + backward + optimize\n",
        "              outputs = net(inputs)\n",
        "              loss = criterion(outputs, labels)\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "              # print statistics\n",
        "              running_loss_train += loss.data[0]\n",
        "              #Adjust learning rate by dividing by sqrt(t)\n",
        "              t = i+epoch*num_batches_train_hyp+1\n",
        "              adjust_learning_rate(optimizer,alpha,t)\n",
        "\n",
        "          train_loss = running_loss_train/num_batches_train_hyp\n",
        "          valid_loss = validation_loss(validloader_mnist_hyp, num_batches_valid_hyp,net, criterion)\n",
        "          print ('Epoch: %d Train loss: %.3f' %(epoch + 1, train_loss))\n",
        "          print ('Epoch: %d Valid loss: %.3f' %(epoch + 1, valid_loss))\n",
        "          train_loss_list[param_combo_count][0].append(train_loss)\n",
        "          valid_loss_list[param_combo_count][0].append(valid_loss)\n",
        "      #print(np.min(valid_loss_list[param_combo_count][0]))\n",
        "      #validlossmin = np.min(valid_loss_list[param_combo_count][0])\n",
        "      validlossend = valid_loss_list[param_combo_count][0][-1]\n",
        "      valid_loss_min_list[param_combo_count][0].append(validlossend)\n",
        "      valid_loss_min_lista[alpha_count][0].append(validlossend)\n",
        "      valid_loss_min_listb[beta_count][0].append(validlossend)\n",
        "\n",
        "      elapsed_time = time.time() - start_time_combo\n",
        "      print('Finished Training') \n",
        "      param_combo_count += 1\n",
        "      print(\"Remaining Time for hyper-parameter tuning of Adam: \", int((total_combo-param_combo_count)*elapsed_time/60),\" minutes\",\n",
        "           int((total_combo-param_combo_count)*elapsed_time%60),\" seconds\")\n",
        "      beta_count += 1\n",
        "    \n",
        "    print(valid_loss_min_lista[alpha_count][0])\n",
        "\n",
        "\n",
        "    alpha_count += 1\n",
        "    print(alpha_count)\n",
        "\n",
        "    \n",
        "   \n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-47v6Ba6X9i-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Plotting all results of hyperparameter tuning for AMSGRAD vs Beta2\n",
        "\n",
        "if hyperparam_tuning_amsgrad:\n",
        "  plt.figure(figsize=(7,5))\n",
        "  #title = \"alpha = \" + str(train_loss_list[i][1]) + \" Beta2 = \" + str(train_loss_list[i][2])\n",
        "  title = \"Hyperparameter Tuning by Minimum Validation Loss for AMSGRAD\"\n",
        "  plt.title(title)\n",
        "  for i in range(len(alphas)):\n",
        "    plt.plot(betas, valid_loss_min_lista[i][0], label=r'$\\alpha = $'+str(alphas[i]))\n",
        "    #plt.ylim(0,1.0)\n",
        "  plt.grid(color='gray', linewidth=1)\n",
        "  plt.legend(loc='best')\n",
        "  plt.xlabel(r'$\\beta_2$')\n",
        "  plt.ylabel('minimum validation loss')\n",
        "  plt.xlim((np.min(betas),np.max(betas)))\n",
        "  plt.xticks(betas)\n",
        "  plt.gca().set_facecolor('w')\n",
        "  plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gdsA0XTBFcSQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Plotting all results of hyperparameter tuning for AMSGRAD vs alpha\n",
        "\n",
        "if hyperparam_tuning_amsgrad:\n",
        "  plt.figure(figsize=(7,5))\n",
        "  #title = \"alpha = \" + str(train_loss_list[i][1]) + \" Beta2 = \" + str(train_loss_list[i][2])\n",
        "  title = \"Hyperparameter Tuning by Minimum Validation Loss for AMSGRAD\"\n",
        "  plt.title(title)\n",
        "  for i in range(len(betas)):\n",
        "    plt.semilogx(alphas, valid_loss_min_listb[i][0], label=r'$\\beta_2 = $'+str(round(betas[i],4))+r'$, \\alpha_{min} = $'+str(round(alphas[np.argmin(valid_loss_min_listb[i][0])],3)))\n",
        "    print(alphas[np.argmin(valid_loss_min_listb[i][0])],betas[i],np.min(valid_loss_min_listb[i][0]))\n",
        "    #plt.ylim(0,1.0)\n",
        "  plt.grid(color='gray', linewidth=1)\n",
        "  plt.legend(loc='best')\n",
        "  plt.xlabel(r'$\\alpha$')\n",
        "  plt.ylabel('Minimum Validation Loss')\n",
        "  plt.xlim((np.min(alphas),np.max(alphas)))\n",
        "  plt.ylim(0,4)\n",
        "  plt.xticks(alphas)\n",
        "  plt.gca().set_facecolor('w')\n",
        "  plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1JcaV10jiIVh",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Train Loss for each set of hyperparameters in AMSGRAD\n",
        "if hyperparam_tuning_amsgrad:\n",
        "  x = num_batches_train_hyp*np.arange(0,num_epochs_hyp)\n",
        "  print(x)\n",
        "  for i in range(len(train_loss_list)):\n",
        "\n",
        "    plt.figure(figsize=(7,5))\n",
        "    title = \"alpha = \" + str(train_loss_list[i][1]) + \" Beta2 = \" + str(train_loss_list[i][2])\n",
        "    plt.title(title)\n",
        "    plt.plot(x, train_loss_list[i][0], 'g', label=\"Amsgrad\")\n",
        "    #plt.ylim(0,1.0)\n",
        "    plt.grid(color='gray', linewidth=1)\n",
        "    plt.legend(loc='best')\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Train loss')\n",
        "    plt.gca().set_facecolor('w')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qCjQjd_8iRdI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "if hyperparam_tuning_amsgrad:\n",
        "  x = num_batches_train_hyp*np.arange(0,num_epochs_hyp)\n",
        "  print(x)\n",
        "  for i in range(len(valid_loss_list)):\n",
        "\n",
        "    plt.figure(figsize=(7,5))\n",
        "    title = \"alpha = \" + str(valid_loss_list[i][1]) + \" Beta2 = \" + str(valid_loss_list[i][2])\n",
        "    plt.title(title)\n",
        "    plt.plot(x, valid_loss_list[i][0], 'g', label=\"Amsgrad\")\n",
        "    plt.grid(color='gray', linewidth=1)\n",
        "    plt.legend(loc='best')\n",
        "    plt.ylim(0,1.3)\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Valid loss')\n",
        "    plt.gca().set_facecolor('w')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yW6y56Y5gggD",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Instanstiating the network on GPU\n",
        "\n",
        "net.load_state_dict(initial_state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n5Lr0B5JvwD2",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#Optimal Hyperparameters for AMSGRAD after hyperparameter tuning\n",
        "alpha_amsgrad = 0.015848931924611134\n",
        "beta2_amsgrad = 0.994"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e_8zGIRagi1d",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Learning the parameters using Amsgrad optimization with categorical cross-entropy loss. The learning rate is initially set to 0.001.\n",
        "optimizer = Amsgrad(net.parameters(), lr=alpha_amsgrad, betas=(0.9,beta2_amsgrad))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O9E-9zQ7oWBo",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "cellView": "code",
        "outputId": "55730439-5077-4ee2-a367-70c839449dc6",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1524268328156,
          "user_tz": 240,
          "elapsed": 114563,
          "user": {
            "displayName": "Hamed Layeghi",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107778543655020009707"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# AMSGrad train and test loss per epoch with found hyperparameters\n",
        "\n",
        "train_loss_list_epoch = []\n",
        "test_loss_list_epoch = []\n",
        "train_loss_list_iter = []\n",
        "pkl_filename = 'nn_saved_model.pkl'\n",
        "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss_train = 0.0\n",
        "    \n",
        "    for i, data in enumerate(trainloader_mnist, 0):\n",
        "        running_loss = 0.0\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "\n",
        "        # wrap them in Variable\n",
        "        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss_train += loss.data[0]\n",
        "        running_loss = loss.data[0]\n",
        "        \n",
        "        t = i+epoch*num_batches_train+1\n",
        "        adjust_learning_rate(optimizer,alpha_amsgrad,t)\n",
        "        train_loss_list_iter.append(running_loss)\n",
        "        \n",
        "    train_loss = running_loss_train/num_batches_train\n",
        "    test_loss = validation_loss(testloader_mnist, num_batches_test, net, criterion)\n",
        "    print ('Epoch: %d Train loss: %.3f' %(epoch + 1, train_loss))\n",
        "    print ('Epoch: %d Test loss: %.3f' %(epoch + 1, test_loss))\n",
        "    train_loss_list_epoch.append(train_loss)\n",
        "    test_loss_list_epoch.append(test_loss)\n",
        "\n",
        "    torch.save(net.state_dict(), pkl_filename)\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 422,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 Train loss: 0.606\n",
            "Epoch: 1 Test loss: 0.058\n",
            "Epoch: 2 Train loss: 0.349\n",
            "Epoch: 2 Test loss: 0.054\n",
            "Epoch: 3 Train loss: 0.330\n",
            "Epoch: 3 Test loss: 0.052\n",
            "Epoch: 4 Train loss: 0.321\n",
            "Epoch: 4 Test loss: 0.052\n",
            "Epoch: 5 Train loss: 0.315\n",
            "Epoch: 5 Test loss: 0.051\n",
            "Epoch: 6 Train loss: 0.311\n",
            "Epoch: 6 Test loss: 0.050\n",
            "Epoch: 7 Train loss: 0.308\n",
            "Epoch: 7 Test loss: 0.050\n",
            "Epoch: 8 Train loss: 0.305\n",
            "Epoch: 8 Test loss: 0.050\n",
            "Epoch: 9 Train loss: 0.303\n",
            "Epoch: 9 Test loss: 0.049\n",
            "Epoch: 10 Train loss: 0.301\n",
            "Epoch: 10 Test loss: 0.050\n",
            "Epoch: 11 Train loss: 0.300\n",
            "Epoch: 11 Test loss: 0.049\n",
            "Epoch: 12 Train loss: 0.298\n",
            "Epoch: 12 Test loss: 0.049\n",
            "Epoch: 13 Train loss: 0.297\n",
            "Epoch: 13 Test loss: 0.049\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WPMhLZ_mGeRG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Instanstiating the network on GPU\n",
        "net_adam = LogisticRegression(input_size, num_classes)\n",
        "net_adam.load_state_dict(initial_state)\n",
        "net_adam = net_adam.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eyK-Z1OOqW3r",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning for ADAM\n",
        "if hyperparam_tuning_adam:\n",
        "  train_loss_list_adam = []\n",
        "  valid_loss_list_adam = []\n",
        "  valid_loss_min_list_adam = []\n",
        "  valid_loss_min_lista_adam = []\n",
        "  valid_loss_min_listb_adam = []\n",
        "  for alpha in alphas_adam:\n",
        "    valid_loss_min_lista_adam.append(([], alpha))\n",
        "    for beta in betas_adam:\n",
        "      train_loss_list_adam.append(([], alpha, beta))\n",
        "      valid_loss_list_adam.append(([], alpha, beta))\n",
        "      valid_loss_min_list_adam.append(([], alpha, beta))\n",
        "  for beta in betas:\n",
        "    valid_loss_min_listb_adam.append(([], beta))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  total_combo = len(alphas_adam)*len(betas_adam)\n",
        "\n",
        "  param_combo_count = 0\n",
        "  alpha_count = 0\n",
        "  for alpha in alphas_adam:\n",
        "    beta_count = 0\n",
        "    for beta in betas_adam:\n",
        "      print(\"alpha:\", alpha, \" beta 2: \", beta)\n",
        "      print(\"Parameter Combination:\", param_combo_count+1, \"/\", total_combo)\n",
        "      net_adam.load_state_dict(initial_state)\n",
        "      optimizer_adam = optim.Adam(net_adam.parameters(), lr=alpha, betas=(0.9,beta))\n",
        "      start_time_combo = time.time()\n",
        "\n",
        "\n",
        "      for epoch in range(num_epochs_hyp):  # loop over the dataset multiple times\n",
        "          running_loss_train_adam = 0.0\n",
        "          for i, data in enumerate(trainloader_mnist_hyp, 0):\n",
        "              # get the inputs\n",
        "              inputs, labels = data\n",
        "\n",
        "              # wrap them in Variable\n",
        "              inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
        "\n",
        "              # zero the parameter gradients\n",
        "              optimizer_adam.zero_grad()\n",
        "\n",
        "              # forward + backward + optimize\n",
        "              outputs = net_adam(inputs)\n",
        "              loss = criterion(outputs, labels)\n",
        "              loss.backward()\n",
        "              optimizer_adam.step()\n",
        "\n",
        "              # print statistics\n",
        "              running_loss_train_adam += loss.data[0]\n",
        "              #Adjust learning rate by dividing by sqrt(t)\n",
        "              t = i+epoch*num_batches_train_hyp+1\n",
        "              adjust_learning_rate(optimizer_adam,alpha,t)\n",
        "\n",
        "          train_loss_adam = running_loss_train_adam/num_batches_train_hyp\n",
        "          valid_loss_adam = validation_loss(validloader_mnist_hyp, num_batches_valid_hyp,net_adam, criterion)\n",
        "          print ('Epoch: %d Train loss: %.3f' %(epoch + 1, train_loss_adam))\n",
        "          print ('Epoch: %d Valid loss: %.3f' %(epoch + 1, valid_loss_adam))\n",
        "          train_loss_list_adam[param_combo_count][0].append(train_loss_adam)\n",
        "          valid_loss_list_adam[param_combo_count][0].append(valid_loss_adam)\n",
        "      #print(np.min(valid_loss_list_adam[param_combo_count][0]))\n",
        "      #validlossmin = np.min(valid_loss_list_adam[param_combo_count][0])\n",
        "      validlossend = valid_loss_list_adam[param_combo_count][0][-1]\n",
        "      valid_loss_min_list_adam[param_combo_count][0].append(validlossend)\n",
        "      valid_loss_min_lista_adam[alpha_count][0].append(validlossend)\n",
        "      valid_loss_min_listb_adam[beta_count][0].append(validlossend)\n",
        "\n",
        "\n",
        "      elapsed_time = time.time() - start_time_combo\n",
        "      print('Finished Training') \n",
        "      param_combo_count += 1\n",
        "      print(\"Remaining Time for hyper-parameter tuning of Adam: \", int((total_combo-param_combo_count)*elapsed_time/60),\" minutes\",\n",
        "           int((total_combo-param_combo_count)*elapsed_time%60),\" seconds\")\n",
        "      beta_count +=1\n",
        "    \n",
        "    print(valid_loss_min_lista_adam[alpha_count][0])\n",
        "    print(valid_loss_min_lista_adam)\n",
        "\n",
        "    alpha_count += 1\n",
        "    print(alpha_count)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qy_X1O4NtPlf",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b9768c5b-e3eb-4fee-b3bf-a393a5afbf54",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1524268329308,
          "user_tz": 240,
          "elapsed": 266,
          "user": {
            "displayName": "Hamed Layeghi",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107778543655020009707"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Plotting all results of hyperparameter tuning for ADAM\n",
        "print (valid_loss_min_lista_adam)\n",
        "if hyperparam_tuning_adam:\n",
        "  plt.figure(figsize=(7,5))\n",
        "  #title = \"alpha = \" + str(train_loss_list[i][1]) + \" Beta2 = \" + str(train_loss_list[i][2])\n",
        "  title = \"Hyperparameter Tuning by Minimum Validation Loss for ADAM\"\n",
        "  plt.title(title)\n",
        "  for i in range(len(alphas_adam)):\n",
        "    plt.plot(betas_adam, valid_loss_min_lista_adam[i][0], label=r'$\\alpha = $'+str(alphas_adam[i]))\n",
        "    #plt.ylim(0,1.0)\n",
        "  plt.grid(color='gray', linewidth=1)\n",
        "  plt.legend(loc='best')\n",
        "  plt.xlabel(r'$\\beta_2$')\n",
        "  plt.ylabel('minimum validation loss')\n",
        "  plt.xlim((np.min(betas),np.max(betas)))\n",
        "  plt.xticks(betas_adam)\n",
        "  plt.gca().set_facecolor('w')\n",
        "  plt.show()\n"
      ],
      "execution_count": 425,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[([0.7797144949436188, 0.9028547525405883, 0.8595818042755127], 0.5623413251903491), ([0.6596395403146744, 0.530584043264389, 0.4747235536575317], 0.31622776601683794), ([0.349319152534008, 0.3438749134540558, 0.38749154806137087], 0.1778279410038923), ([0.30779177248477935, 0.3047154262661934, 0.31403210312128066], 0.1), ([0.3219259589910507, 0.3041494287550449, 0.29674054533243177], 0.05623413251903491), ([0.2993109107017517, 0.28934305533766747, 0.2919121146202087], 0.03162277660168379), ([0.29180932268500326, 0.2950167149305344, 0.2977304995059967], 0.01778279410038923), ([0.30477443635463713, 0.30719234645366666, 0.311457759141922], 0.01)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "baSxz7k9GVec",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "if hyperparam_tuning_adam:\n",
        "  plt.figure(figsize=(7,5))\n",
        "  #title = \"alpha = \" + str(train_loss_list[i][1]) + \" Beta2 = \" + str(train_loss_list[i][2])\n",
        "  title = \"Hyperparameter Tuning by Minimum Validation Loss for ADAM\"\n",
        "  plt.title(title)\n",
        "  for i in range(len(betas_adam)):\n",
        "    plt.semilogx(alphas_adam, valid_loss_min_listb_adam[i][0], label=r'$\\beta_2 = $'+str(round(betas_adam[i],4))+r'$, \\alpha_{min} = $'+str(round(alphas_adam[np.argmin(valid_loss_min_listb_adam[i][0])],3)))\n",
        "    print(alphas_adam[np.argmin(valid_loss_min_listb_adam[i][0])],betas_adam[i],np.min(valid_loss_min_listb_adam[i][0]))\n",
        "\n",
        "    #plt.ylim(0,1.0)\n",
        "  plt.grid(color='gray', linewidth=1)\n",
        "  plt.legend(loc='best')\n",
        "  plt.xlabel(r'$\\alpha$')\n",
        "  plt.ylabel('Minimum Validation Loss')\n",
        "  plt.xlim((np.min(alphas_adam),np.max(alphas_adam)))\n",
        "  plt.ylim(0,4)\n",
        "  plt.xticks(alphas_adam)\n",
        "  plt.gca().set_facecolor('w')\n",
        "  plt.show()\n",
        "\n",
        "# 0.021544346900318846 0.99 0.28360784649848936\n",
        "#0.021544346900318846 0.9925 0.2784189060330391\n",
        "#0.021544346900318846 0.995 0.27910616546869277\n",
        "#0.021544346900318846 0.997 0.29932574182748795\n",
        "#0.01 0.999 0.298560106754303"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gl5I6-3tjZVx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Train Loss for each set of hyperparameters in ADAM\n",
        "\n",
        "if hyperparam_tuning_adam:\n",
        "  x = num_batches_train_hyp*np.arange(0,num_epochs_hyp)\n",
        "  print(x)\n",
        "  for i in range(len(train_loss_list_adam)):\n",
        "\n",
        "    plt.figure(figsize=(7,5))\n",
        "    title = \"alpha = \" + str(train_loss_list_adam[i][1]) + \" Beta2 = \" + str(train_loss_list_adam[i][2])\n",
        "    plt.title(title)\n",
        "    plt.plot(x, train_loss_list_adam[i][0], 'g', label=\"Adam\")\n",
        "    #plt.ylim(0,0.3)\n",
        "    plt.grid(color='gray', linewidth=1)\n",
        "    plt.legend(loc='best')\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Train loss')\n",
        "    plt.gca().set_facecolor('w')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yJIibbmwjbwd",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Test Loss for each set of hyperparameters in ADAM\n",
        "\n",
        "\n",
        "if hyperparam_tuning_adam:\n",
        "  x = num_batches_train_hyp*np.arange(0,num_epochs_hyp)\n",
        "  print(x)\n",
        "  for i in range(len(valid_loss_list_adam)):\n",
        "\n",
        "    plt.figure(figsize=(7,5))\n",
        "    title = \"alpha = \" + str(valid_loss_list_adam[i][1]) + \" Beta2 = \" + str(valid_loss_list_adam[i][2])\n",
        "    plt.title(title)\n",
        "    plt.plot(x, valid_loss_list_adam[i][0], 'g', label=\"Adam\")\n",
        "    plt.grid(color='gray', linewidth=1)\n",
        "    plt.legend(loc='best')\n",
        "    plt.ylim(0,1.3)\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Valid loss')\n",
        "    plt.gca().set_facecolor('w')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vOzWlGmwjeSm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "net_adam.load_state_dict(initial_state) #Setting the initial state of adam to the same values as amsgrad's initial states"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DO1fXmMcvf_G",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Optimal Hyperparameters from Tuning for ADAM\n",
        "alpha_adam = 0.03162277660168379\n",
        "beta2_adam = 0.995"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-jYuI4MgjgGv",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Learning the parameters using Amsgrad optimization with categorical cross-entropy loss. The learning rate is initially set to 0.001.\n",
        "optimizer_adam = optim.Adam(net_adam.parameters(), lr=alpha_adam, betas=(0.9,beta2_adam))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BR1TXtBWjj95",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "outputId": "0730fdfe-c4a2-480b-ff95-26196e13b596",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1524268446224,
          "user_tz": 240,
          "elapsed": 113896,
          "user": {
            "displayName": "Hamed Layeghi",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107778543655020009707"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Adam train and test loss per epoch with found hyperparameters\n",
        "train_loss_list_epoch_adam = []\n",
        "test_loss_list_epoch_adam = []\n",
        "train_loss_list_iter_adam = []\n",
        "pkl_filename = 'nn_saved_model_adam.pkl'\n",
        "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss_train = 0.0\n",
        "    for i, data in enumerate(trainloader_mnist, 0):\n",
        "        running_loss = 0.0\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "\n",
        "        # wrap them in Variable\n",
        "        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer_adam.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net_adam(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_adam.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss_train += loss.data[0]\n",
        "        t = i+epoch*num_batches_train+1\n",
        "        adjust_learning_rate(optimizer_adam,alpha_adam,t)\n",
        "        running_loss = loss.data[0]\n",
        "        train_loss_list_iter_adam.append(running_loss)\n",
        "        \n",
        "    train_loss = running_loss_train/num_batches_train\n",
        "    test_loss = validation_loss(testloader_mnist, num_batches_test, net_adam, criterion)\n",
        "    print ('Epoch: %d Train loss: %.3f' %(epoch + 1, train_loss))\n",
        "    print ('Epoch: %d Valid loss: %.3f' %(epoch + 1, test_loss))\n",
        "    train_loss_list_epoch_adam.append(train_loss)\n",
        "    test_loss_list_epoch_adam.append(test_loss)\n",
        "\n",
        "    torch.save(net_adam.state_dict(), pkl_filename)\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 432,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 Train loss: 0.671\n",
            "Epoch: 1 Valid loss: 0.053\n",
            "Epoch: 2 Train loss: 0.326\n",
            "Epoch: 2 Valid loss: 0.052\n",
            "Epoch: 3 Train loss: 0.306\n",
            "Epoch: 3 Valid loss: 0.049\n",
            "Epoch: 4 Train loss: 0.296\n",
            "Epoch: 4 Valid loss: 0.049\n",
            "Epoch: 5 Train loss: 0.291\n",
            "Epoch: 5 Valid loss: 0.047\n",
            "Epoch: 6 Train loss: 0.285\n",
            "Epoch: 6 Valid loss: 0.047\n",
            "Epoch: 7 Train loss: 0.283\n",
            "Epoch: 7 Valid loss: 0.048\n",
            "Epoch: 8 Train loss: 0.278\n",
            "Epoch: 8 Valid loss: 0.047\n",
            "Epoch: 9 Train loss: 0.276\n",
            "Epoch: 9 Valid loss: 0.047\n",
            "Epoch: 10 Train loss: 0.274\n",
            "Epoch: 10 Valid loss: 0.046\n",
            "Epoch: 11 Train loss: 0.273\n",
            "Epoch: 11 Valid loss: 0.046\n",
            "Epoch: 12 Train loss: 0.271\n",
            "Epoch: 12 Valid loss: 0.046\n",
            "Epoch: 13 Train loss: 0.270\n",
            "Epoch: 13 Valid loss: 0.046\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q-2RQDTnyzy-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "1f373f70-80ce-4259-f90d-86cd160d285e",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1524268446841,
          "user_tz": 240,
          "elapsed": 564,
          "user": {
            "displayName": "Hamed Layeghi",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107778543655020009707"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Train loss\n",
        "x = num_batches_train*np.arange(0,num_epochs)\n",
        "plt.figure(figsize=(7,5))\n",
        "actual, = plt.plot(x,train_loss_list_epoch, 'g', label=\"Amsgrad\")\n",
        "predicted, = plt.plot(x, train_loss_list_epoch_adam, 'b--', label =\"Adams\")\n",
        "#t.title(\"Train loss with number of epochs\")\n",
        "plt.legend(handles=[actual,predicted])\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Train Loss')\n",
        "plt.grid(color='gray', linewidth=1)\n",
        "plt.gca().set_facecolor('w')\n",
        "plt.xlim(0,5000)\n",
        "plt.show()"
      ],
      "execution_count": 433,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcgAAAE9CAYAAACP/vJAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nioiIOPD/Y/tcumFT/PMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fb3028b53c8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "jUl1G3uezsk3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "d8ad9698-7999-49fb-9aa8-73f94b741c85",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1524268447311,
          "user_tz": 240,
          "elapsed": 401,
          "user": {
            "displayName": "Hamed Layeghi",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107778543655020009707"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "x = num_batches_train*np.arange(0,num_epochs)\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "actual, = plt.plot(x,test_loss_list_epoch, 'g', label=\"Amsgrad\")\n",
        "predicted, = plt.plot(x, test_loss_list_epoch_adam, 'b--', label =\"Adams\")\n",
        "#plt.title(\"Validation loss with number of epochs\")\n",
        "plt.legend(handles=[actual,predicted])\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Test Loss')\n",
        "plt.grid(color='gray', linewidth=1)\n",
        "plt.gca().set_facecolor('w')\n",
        "plt.xlim(0,5000)\n",
        "plt.show()"
      ],
      "execution_count": 434,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAc4AAAE9CAYAAACC4IIHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUREpMH/AL5ffCScOtbnAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fb301378dd8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}